{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Management in Dask\n",
    "\n",
    "This tutorial provides a summary of how to manage and utilise the CPU and memory effectively when dealing with large datasets and/or with intensive computation demands. There are several ways of managing larger than memory data and improving code efficiencies. The approaches below are demonstrated with some brief examples.\n",
    "    \n",
    "- Partition\n",
    "    - dask.dataframe\n",
    "    - dask.dataarray\n",
    "- Save data onto disk\n",
    "    - export intermediate data onto disk \n",
    "- Scheduler\n",
    "    - Persist / Compute methods\n",
    "    - Futures as pointers to remote data\n",
    "    - Delayed\n",
    "- Clear data\n",
    "- Execution in background\n",
    "    - Network communication example\n",
    "    \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Authors: NCI Virtual Research Environment Team\n",
    "- Keywords: Dask, Resource Management, Partition, Schedular\n",
    "- Creation Date: 2020-Sep\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Partition \n",
    "\n",
    "Dask operates on chunks. Like what we see in examples of dask arrays and dataframes, Dask provides a parallel, larger-than-memory mechanism using blocked algorithms. Simply put: distributed operation on data series, frame, or array.\n",
    "\n",
    "*  **Parallel**: Uses all of the cores on your computer\n",
    "*  **Larger-than-memory**:  Lets you work on datasets that are larger than your available memory by breaking up your data into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "*  **Blocked Algorithms**:  Perform large computations by performing many smaller computations\n",
    "\n",
    "A Dask DataFrame is composed of many pandas DataFrames. For `dask.dataframe` the chunking happens only along the index.\n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"30%\">\n",
    "\n",
    "Dask arrays coordinate many Numpy arrays, arranged into chunks within a grid. They support a large subset of the Numpy API.\n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-array-black-text.svg\">\n",
    "\n",
    "In this notebook, we'll build some understanding by implementing some blocked algorithms from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "filename = os.path.join('/g/data/dk92/notebooks/demo_data/Weather_Stations_ACT','IDCJAC0009_*_*','IDCJAC0009*.csv')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "ddf = dd.read_csv(filename)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.groupby(\"Product code\")[\"Rainfall amount (millimetres)\"].max().visualize(filename='dataframe_graph.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask.dataarray\n",
    "\n",
    "To open multiple files simultaneously in parallel using Dask delayed, use `open_mfdataset()`.\n",
    "\n",
    "This function will automatically concatenate and merge datasets into one in the simple cases that it understands (see `combine_by_coords()` for the full disclaimer). By default, `open_mfdataset()` will chunk each netCDF file into a single Dask array; again, supply the chunks argument to control the size of the resulting Dask arrays. In more complex cases, you can open each file individually using `open_dataset()` and merge the result, as described in the Combining data xarray tutorial. Passing the keyword argument `parallel=True` to `open_mfdataset()` will speed up the reading of large multi-file datasets by executing those read tasks in parallel using `dask.delayed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask.array as da\n",
    "path = '/g/data/oi10/replicas/CMIP6/ScenarioMIP/NOAA-GFDL/GFDL-CM4/ssp585/r1i1p1f1/day/pr/gr1/v20180701/*'\n",
    "f_ssp585 = xr.open_mfdataset(path)\n",
    "# Use Dask.Distributed utility function to display size of each dataset\n",
    "from distributed.utils import format_bytes\n",
    "print(\n",
    "    \"ssp585:\",\n",
    "    format_bytes(f_ssp585.nbytes),\n",
    ")\n",
    "dsets = xr.open_mfdataset(path,chunks={'time':730},parallel=True)\n",
    "dsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that printing a dataset still shows a preview of array values, even if they are actually Dask arrays. We can do this quickly with Dask because we only need to compute the first few values (typically from the first block). To reveal the true nature of an array, print a DataArray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've manipulated a Dask array, you can still write a dataset too big to fit into memory back to disk by using `to_netcdf()` in the usual way. Be mindful the following cell will take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a location you have write access to\n",
    "dsets.to_netcdf(\"/g/data/dk92/notebooks/demo_data/cmip6-precipitation-data.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, by setting the compute argument to `False`, `to_netcdf()` will return a `dask.delayed` object that can be computed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# or distributed.progress when using the distributed scheduler\n",
    "delayed_obj = dsets.to_netcdf(\"/g/data/dk92/notebooks/demo_data/cmip6-precipitation-data.nc\", compute=False)\n",
    "\n",
    "with ProgressBar():\n",
    "    results = delayed_obj.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do a small scale caculation, reading the whole file into memory is intuitive, but this approach often does not scale. Then segmentation and working on big data bit by bit is a good practice when dealing with larger-than-memory data.  Note that there is often a trade-off between time-efficiency and memory footprint: the following uses very little memory, but may be slower for files that do not fill a large faction of memory. In general, one would like chunks small enough not to stress memory, but big enough for efficient use of the CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save data onto disk\n",
    "\n",
    "Whenever we operate on our data we read through all of our data so that we donâ€™t fill up RAM. This is very efficient for memory use, but reading through all of the data files every time can be slow.\n",
    "\n",
    "As you saw in the dask dataframes example, we stored our data in Parquet, a format that is more efficient for computers to read and write. It is binary file format. Parquet stores nested data structures in a flat columnar format. Compared to a traditional approach where data is stored in row-oriented approach, parquet is more efficient in terms of storage and performance.\n",
    "\n",
    "The following code was copied from that example to remind you one way of how to save data onto disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write data as parquet format\n",
    "ddf.to_parquet('/g/data/dk92/notebooks/demo_data/ACT_weather.parquet', engine='pyarrow')\n",
    "!ls /g/data/dk92/notebooks/demo_data/ACT_weather.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract binary files from disk and do some calcuation with a better performance gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "import pandas as pd\n",
    "df = pd.read_parquet('/g/data/dk92/notebooks/demo_data/ACT_weather.parquet', engine='pyarrow')\n",
    "df[\"Rainfall amount (millimetres)\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scheduler\n",
    "\n",
    "### Persist sends work to the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you run this notebook on your local computer or NCI's VDI instance, you can create cluster\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this notebook on Gadi under pangeo environment, you can create cluster using scheduler.json file\n",
    "from dask.distributed import Client, LocalCluster\n",
    "client = Client(scheduler_file='scheduler.json')\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Warning: Please make sure you specify the correct path to the scheduler.json file within your environment.</b>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the Dask Client will provide a dashboard which is useful to gain insight into the computation. The link to the dashboard will become visible when you create the Client. We recommend having the Client open on one side of your screen and your notebook open on the other side, which will be useful for learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv files again\n",
    "import os\n",
    "import dask\n",
    "filename = os.path.join('/g/data/dk92/notebooks/demo_data/Weather_Stations_ACT','IDCJAC0009_*_*','IDCJAC0009*.csv')\n",
    "import dask.dataframe as dd\n",
    "ddf = dd.read_csv(filename)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf[ddf[\"Rainfall amount (millimetres)\"]> 20]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.persist(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Futures Point to Remote Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import futures_of\n",
    "\n",
    "futures_of(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask holds onto data for which a future exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed feature provides non-Dask results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum().visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Rainfall amount (millimetres)\"].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clearing data\n",
    "\n",
    "We remove data from distributed RAM by removing the collection from our local process. Remote data is removed once all Futures pointing to that data are removed from all client machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df  # Deleting local data often deletes remote data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is the only copy then this will likely trigger the cluster to delete the data as well.\n",
    "\n",
    "However if we have multiple copies or other collections based on this one then weâ€™ll have to delete them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(filename)\n",
    "df2 = df[df[\"Rainfall amount (millimetres)\"] < 10]\n",
    "del df2  # would not delete data, because df2 still tracks the futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggressively Clearing Data\n",
    "\n",
    "To definitely remove a computation and all computations that depend on it you can always cancel the futures/collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(df)  # kills df, df2, and every other dependent computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you want a clean slate, you can restart the cluster. This clears all state and does a hard restart of all worker processes. It generally completes in around a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execution in the background\n",
    "\n",
    "There are many tasks that take a while to complete, but don't actually require much of the CPU, for example anything that requires communication over a network, or input from a user. In typical sequential programming, execution would need to halt while the process completes, and then continue execution. That would be dreadful for a user experience (imagine the slow progress bar that locks up the application and cannot be cancelled), and wasteful of time (the CPU could have been doing useful work in the meantime).\n",
    "For example, we can launch processes and get their output as follows:\n",
    "```python\n",
    "    import subprocess\n",
    "    p = subprocess.Popen(command, stdout=subprocess.PIPE)\n",
    "    p.returncode\n",
    "```\n",
    "The task is run in a separate process, and the return-code will remain `None` until it completes, when it will change to `0`. To get the result back, we need `out = p.communicate()[0]` (which would block if the process was not complete).\n",
    "\n",
    "Similarly, we can launch Python processes and threads in the background. Some methods allow mapping over multiple inputs and gathering the results. The thread starts and the cell completes immediately, but the data associated with the download only appears in the queue object some time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell calls data as a remote file using the OPeNDAP protocol\n",
    "\n",
    "import threading\n",
    "import queue\n",
    "import urllib\n",
    "\n",
    "def get_webdata(url, q):\n",
    "    u = urllib.request.urlopen(url)\n",
    "    # raise ValueError\n",
    "    q.put(u.read())\n",
    "\n",
    "q = queue.Queue()\n",
    "t = threading.Thread(target=get_webdata, args=('http://dapds00.nci.org.au/thredds/dodsC/rr3/CMIP5/output1/CSIRO-BOM/ACCESS1-0/1pctCO2/day/atmos/day/r1i1p1/latest/pr/pr_day_ACCESS1-0_1pctCO2_r1i1p1_03000101-03241231.nc.html', q))\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the cell above won't work if you run this using Pangeo on Gadi as the compute notes do have external internet access, but will work on VDI, login nodes, or local PC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch result back into this thread. If the worker thread is not done, this would wait.\n",
    "q.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider:** What would you see if there had been an exception within the `get_webdata` function? You could uncomment the `raise` line, above, and re-execute the two cells. What happens? Is there any way to debug the execution to find the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This example summarizes some basic strategies of using chunks, dask lazy excution, schedulers, futures and persist utilities for a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "https://docs.dask.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
